{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Processing\n",
    "In this notebook the data will be cleaned.\n",
    "\n",
    "We are using two different datasets for this project: the Bikesharing Dataset and the Weather Dataset. We analysed the datasets regarding null values, added additional data, addressed missing data, and dropped outliers and duplicates.\n",
    "\n",
    "This notebook is structured as follows:\n",
    "\n",
    "**Bikesharing Dataset**\n",
    "* Importing the Dataset and the Libraries needed for processing\n",
    "* Additional date related Features\n",
    "* Additional Datasets\n",
    "* Missing Data in the extension set\n",
    "* Dropping outliers and redundant features\n",
    "\n",
    "**Weather Dataset**\n",
    "* Extending the given weather related dataframe and filling missing values\n",
    "* Additional date related Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bikesharing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Dataset and the libraries needed for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime, timedelta, date, time\n",
    "from haversine import haversine\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "la_2018_set = pd.read_csv(\"data/la_2018.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>user_type</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>end_station_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 00:04:00</td>\n",
       "      <td>2018-01-01 00:25:00</td>\n",
       "      <td>3063</td>\n",
       "      <td>3018</td>\n",
       "      <td>5889</td>\n",
       "      <td>Walk-up</td>\n",
       "      <td>Pershing Square</td>\n",
       "      <td>Grand &amp; Olympic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 00:05:00</td>\n",
       "      <td>2018-01-01 00:25:00</td>\n",
       "      <td>3063</td>\n",
       "      <td>3018</td>\n",
       "      <td>6311</td>\n",
       "      <td>Walk-up</td>\n",
       "      <td>Pershing Square</td>\n",
       "      <td>Grand &amp; Olympic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 00:06:00</td>\n",
       "      <td>2018-01-01 00:25:00</td>\n",
       "      <td>3063</td>\n",
       "      <td>3018</td>\n",
       "      <td>5753</td>\n",
       "      <td>Walk-up</td>\n",
       "      <td>Pershing Square</td>\n",
       "      <td>Grand &amp; Olympic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 00:13:00</td>\n",
       "      <td>2018-01-01 00:35:00</td>\n",
       "      <td>3018</td>\n",
       "      <td>3031</td>\n",
       "      <td>6220</td>\n",
       "      <td>Monthly Pass</td>\n",
       "      <td>Grand &amp; Olympic</td>\n",
       "      <td>7th &amp; Spring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 00:14:00</td>\n",
       "      <td>2018-01-01 00:59:00</td>\n",
       "      <td>4204</td>\n",
       "      <td>4216</td>\n",
       "      <td>12436</td>\n",
       "      <td>Monthly Pass</td>\n",
       "      <td>Washington &amp; Abbot Kinney</td>\n",
       "      <td>17th St / SMC E Line Station</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            start_time             end_time  start_station_id  end_station_id  \\\n",
       "0  2018-01-01 00:04:00  2018-01-01 00:25:00              3063            3018   \n",
       "1  2018-01-01 00:05:00  2018-01-01 00:25:00              3063            3018   \n",
       "2  2018-01-01 00:06:00  2018-01-01 00:25:00              3063            3018   \n",
       "3  2018-01-01 00:13:00  2018-01-01 00:35:00              3018            3031   \n",
       "4  2018-01-01 00:14:00  2018-01-01 00:59:00              4204            4216   \n",
       "\n",
       "   bike_id     user_type         start_station_name  \\\n",
       "0     5889       Walk-up            Pershing Square   \n",
       "1     6311       Walk-up            Pershing Square   \n",
       "2     5753       Walk-up            Pershing Square   \n",
       "3     6220  Monthly Pass            Grand & Olympic   \n",
       "4    12436  Monthly Pass  Washington & Abbot Kinney   \n",
       "\n",
       "               end_station_name  \n",
       "0               Grand & Olympic  \n",
       "1               Grand & Olympic  \n",
       "2               Grand & Olympic  \n",
       "3                  7th & Spring  \n",
       "4  17th St / SMC E Line Station  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la_2018_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename some columns for the sake of convenience\n",
    "la_2018_set.rename(columns = {'start_station_id':'start_station'}, inplace = True)\n",
    "la_2018_set.rename(columns = {'end_station_id':'end_station'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 311894 entries, 0 to 311893\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   start_time          311894 non-null  object\n",
      " 1   end_time            311894 non-null  object\n",
      " 2   start_station       311894 non-null  int64 \n",
      " 3   end_station         311894 non-null  int64 \n",
      " 4   bike_id             311894 non-null  int64 \n",
      " 5   user_type           311894 non-null  object\n",
      " 6   start_station_name  311894 non-null  object\n",
      " 7   end_station_name    311894 non-null  object\n",
      "dtypes: int64(3), object(5)\n",
      "memory usage: 19.0+ MB\n"
     ]
    }
   ],
   "source": [
    "la_2018_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional date related Features\n",
    "As we want to gain deep insight into the Bikesharing business at specific timeframes we want to generate additional date related features. We can do this by casting the start- and end time to datetime objects and adding date related collumns for the hour, weekday, day, and month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast start_time & end_time to datetime objects\n",
    "la_2018_set[\"start_time\"] = pd.to_datetime(la_2018_set[\"start_time\"])\n",
    "la_2018_set[\"end_time\"] = pd.to_datetime(la_2018_set[\"end_time\"])\n",
    "\n",
    "# Add some date related collumns to the dataset\n",
    "la_2018_set[\"hour\"] = la_2018_set[\"start_time\"].apply(lambda x: x.hour)\n",
    "la_2018_set[\"week_day\"] = la_2018_set[\"start_time\"].apply(lambda x: x.weekday())\n",
    "la_2018_set[\"day\"] = la_2018_set[\"start_time\"].apply(lambda x: x.strftime(\"%d/%m/%Y\"))\n",
    "la_2018_set[\"month\"] = la_2018_set[\"start_time\"].apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additonal Datasets\n",
    "Additional data was available on the [Los Angeles Bikeshare Metro website](https://bikeshare.metro.net/about/data/). We will use this data to improve our ability to visualize the data and predict future demand. Since the data was splitted into quarters, we must first combine it into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>trip_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>start_station</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lon</th>\n",
       "      <th>end_station</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lon</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>plan_duration</th>\n",
       "      <th>trip_route_category</th>\n",
       "      <th>passholder_type</th>\n",
       "      <th>bike_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>65406367</td>\n",
       "      <td>21</td>\n",
       "      <td>2018-01-01 00:04:00</td>\n",
       "      <td>2018-01-01 00:25:00</td>\n",
       "      <td>3063</td>\n",
       "      <td>34.049198</td>\n",
       "      <td>-118.252831</td>\n",
       "      <td>3018</td>\n",
       "      <td>34.043732</td>\n",
       "      <td>-118.260139</td>\n",
       "      <td>5889</td>\n",
       "      <td>0</td>\n",
       "      <td>One Way</td>\n",
       "      <td>Walk-up</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>65406366</td>\n",
       "      <td>20</td>\n",
       "      <td>2018-01-01 00:05:00</td>\n",
       "      <td>2018-01-01 00:25:00</td>\n",
       "      <td>3063</td>\n",
       "      <td>34.049198</td>\n",
       "      <td>-118.252831</td>\n",
       "      <td>3018</td>\n",
       "      <td>34.043732</td>\n",
       "      <td>-118.260139</td>\n",
       "      <td>6311</td>\n",
       "      <td>0</td>\n",
       "      <td>One Way</td>\n",
       "      <td>Walk-up</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>65406365</td>\n",
       "      <td>19</td>\n",
       "      <td>2018-01-01 00:06:00</td>\n",
       "      <td>2018-01-01 00:25:00</td>\n",
       "      <td>3063</td>\n",
       "      <td>34.049198</td>\n",
       "      <td>-118.252831</td>\n",
       "      <td>3018</td>\n",
       "      <td>34.043732</td>\n",
       "      <td>-118.260139</td>\n",
       "      <td>5753</td>\n",
       "      <td>0</td>\n",
       "      <td>One Way</td>\n",
       "      <td>Walk-up</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>65406364</td>\n",
       "      <td>22</td>\n",
       "      <td>2018-01-01 00:13:00</td>\n",
       "      <td>2018-01-01 00:35:00</td>\n",
       "      <td>3018</td>\n",
       "      <td>34.043732</td>\n",
       "      <td>-118.260139</td>\n",
       "      <td>3031</td>\n",
       "      <td>34.044701</td>\n",
       "      <td>-118.252441</td>\n",
       "      <td>6220</td>\n",
       "      <td>30</td>\n",
       "      <td>One Way</td>\n",
       "      <td>Monthly Pass</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>65406362</td>\n",
       "      <td>45</td>\n",
       "      <td>2018-01-01 00:14:00</td>\n",
       "      <td>2018-01-01 00:59:00</td>\n",
       "      <td>4204</td>\n",
       "      <td>33.988419</td>\n",
       "      <td>-118.451630</td>\n",
       "      <td>4216</td>\n",
       "      <td>34.023392</td>\n",
       "      <td>-118.479637</td>\n",
       "      <td>12436</td>\n",
       "      <td>30</td>\n",
       "      <td>One Way</td>\n",
       "      <td>Monthly Pass</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index   trip_id  duration           start_time             end_time  \\\n",
       "0      0  65406367        21  2018-01-01 00:04:00  2018-01-01 00:25:00   \n",
       "1      1  65406366        20  2018-01-01 00:05:00  2018-01-01 00:25:00   \n",
       "2      2  65406365        19  2018-01-01 00:06:00  2018-01-01 00:25:00   \n",
       "3      3  65406364        22  2018-01-01 00:13:00  2018-01-01 00:35:00   \n",
       "4      4  65406362        45  2018-01-01 00:14:00  2018-01-01 00:59:00   \n",
       "\n",
       "   start_station  start_lat   start_lon  end_station    end_lat     end_lon  \\\n",
       "0           3063  34.049198 -118.252831         3018  34.043732 -118.260139   \n",
       "1           3063  34.049198 -118.252831         3018  34.043732 -118.260139   \n",
       "2           3063  34.049198 -118.252831         3018  34.043732 -118.260139   \n",
       "3           3018  34.043732 -118.260139         3031  34.044701 -118.252441   \n",
       "4           4204  33.988419 -118.451630         4216  34.023392 -118.479637   \n",
       "\n",
       "   bike_id  plan_duration trip_route_category passholder_type bike_type  \n",
       "0     5889              0             One Way         Walk-up       NaN  \n",
       "1     6311              0             One Way         Walk-up       NaN  \n",
       "2     5753              0             One Way         Walk-up       NaN  \n",
       "3     6220             30             One Way    Monthly Pass       NaN  \n",
       "4    12436             30             One Way    Monthly Pass       NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all four datasets for every quarter\n",
    "la_q1_set = pd.read_csv(\"data/metro-bike-share-trips-2018-q1.csv\")\n",
    "la_q2_set = pd.read_csv(\"data/metro-bike-share-trips-2018-q2.csv\")\n",
    "la_q3_set = pd.read_csv(\"data/metro-bike-share-trips-2018-q3.csv\")\n",
    "la_q4_set = pd.read_csv(\"data/metro-bike-share-trips-2018-q4.csv\")\n",
    "\n",
    "# Combine all four datasets into one dataframe. It is named as an extension to our main dataframe\n",
    "la_2018_extension = pd.concat([la_q1_set,la_q2_set,la_q3_set,la_q4_set])\n",
    "la_2018_extension = la_2018_extension.reset_index()\n",
    "la_2018_extension.drop(\"index\", axis=1)\n",
    "la_2018_extension.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data in the extension set\n",
    "As we can see, there are some columns with missing data (null values). The affected columns include: start_lat, start_lon, end_lat, end_lon and bike_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 311894 entries, 0 to 311893\n",
      "Data columns (total 16 columns):\n",
      " #   Column               Non-Null Count   Dtype  \n",
      "---  ------               --------------   -----  \n",
      " 0   index                311894 non-null  int64  \n",
      " 1   trip_id              311894 non-null  int64  \n",
      " 2   duration             311894 non-null  int64  \n",
      " 3   start_time           311894 non-null  object \n",
      " 4   end_time             311894 non-null  object \n",
      " 5   start_station        311894 non-null  int64  \n",
      " 6   start_lat            311146 non-null  float64\n",
      " 7   start_lon            311146 non-null  float64\n",
      " 8   end_station          311894 non-null  int64  \n",
      " 9   end_lat              306771 non-null  float64\n",
      " 10  end_lon              306771 non-null  float64\n",
      " 11  bike_id              311894 non-null  int64  \n",
      " 12  plan_duration        311894 non-null  int64  \n",
      " 13  trip_route_category  311894 non-null  object \n",
      " 14  passholder_type      311894 non-null  object \n",
      " 15  bike_type            73867 non-null   object \n",
      "dtypes: float64(4), int64(7), object(5)\n",
      "memory usage: 38.1+ MB\n"
     ]
    }
   ],
   "source": [
    "la_2018_extension.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can add the duration & trip_id feature directly to our main dataset la_2018_set, since they don't contain null values.\n",
    "la_2018_set[\"trip_id\"] = la_2018_extension[\"trip_id\"]\n",
    "la_2018_set[\"duration\"] = la_2018_extension[\"duration\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A look at the units with missing coordinates reveals that these enteties belong to a specific bike station (3000). According to the [Los Angeles Bikeshare Metro website](https://bikeshare.metro.net/about/data/), this station is a \"Virtual Station\" used by employees to check in or check out a bike remotely for a special event or in a situation in which a bike could not otherwise be checked in or out to a station. As we want to gain knowledge about popular routes and distances driven, we need the start/end coordinates of a trip as a tuple. Since no start/end coordinates are specified for the entities with start/end station 3000, we decided to fill these entities assuming that the missing values correspond to the existing values (start/end station specified). Since we cannot use haversine when the starting station is equal to the ending station, we decided to fill these value with the mean of the remaining values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Combine longitude/latitude\n",
    "la_2018_extension[\"start_coordinates\"] = list(zip(la_2018_extension[\"start_lat\"].round(4),la_2018_extension[\"start_lon\"].round(4)))\n",
    "la_2018_extension[\"end_coordinates\"] = list(zip(la_2018_extension[\"end_lat\"].round(4),la_2018_extension[\"end_lon\"].round(4)))\n",
    "\n",
    "# Add Collums to our main dataframe\n",
    "la_2018_set[\"start_coordinates\"] = la_2018_extension[\"start_coordinates\"]\n",
    "la_2018_set[\"end_coordinates\"] = la_2018_extension[\"end_coordinates\"]\n",
    "\n",
    "# Calculating the distances driven (using Haversine if the coordinates are available) OTHERWISE -1 IS RETURNED!\n",
    "# And adding the distance to our main dataframe\n",
    "def calculateDistance(x):\n",
    "    \n",
    "    if(math.isnan(x[\"start_coordinates\"][0])|math.isnan(x[\"start_coordinates\"][1])|math.isnan(x[\"end_coordinates\"][0])|math.isnan(x[\"end_coordinates\"][1])|(x[\"start_coordinates\"]==x[\"end_coordinates\"])): return -1\n",
    "    return haversine(x[\"start_coordinates\"], x[\"end_coordinates\"])\n",
    "\n",
    "la_2018_set[\"distance\"] = la_2018_set.apply(lambda x: calculateDistance(x), axis=1)\n",
    "\n",
    "# Next, we want to get rid of the -1 distances. Since most entities now have a valid distance, we can calculate the mean value. We will then use this mean value as a replacement for the -1 values\n",
    "distance_per_minute_sum = 0\n",
    "valid_distances = 0\n",
    "\n",
    "for index, row in la_2018_set.iterrows():\n",
    "    \n",
    "    if(row[\"distance\"] > -1):\n",
    "        distance_per_minute_sum = distance_per_minute_sum + (row[\"distance\"] / row[\"duration\"])\n",
    "        valid_distances = valid_distances + 1\n",
    "        \n",
    "mean_distance_per_minute = distance_per_minute_sum / valid_distances\n",
    "\n",
    "la_2018_set[\"distance\"] = la_2018_set.apply(lambda x: np.round(x[\"distance\"],2) if x[\"distance\"] > 0 else np.round(mean_distance_per_minute * x[\"duration\"],2), axis=1)\n",
    "\n",
    "# Calculate the speed and add it to our main dataframe as \"km/h\"\n",
    "la_2018_set[\"km/h\"] = la_2018_set.apply(lambda x: np.round(x[\"distance\"]/(x[\"duration\"]/60),1), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another factor of interest is the type of bike ridden, as prices differ between standard and electric bikes. We found that for some entities we can draw conclusions about the bike_type, since some bike_ids provide information about what type of bike it is. Unfortunately, this is not the case for all entities, so we assumed that the missing values correspond to the more common standard bike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 5100,  5746,  5756,  5763,  5772,  5785,  5827,  5828,  5832,\n",
       "        5850,  5857,  5862,  5864,  5873,  5880,  5891,  5897,  5933,\n",
       "        5956,  5957,  5959,  5968,  5983,  6007,  6015,  6021,  6035,\n",
       "        6040,  6041,  6056,  6057,  6063,  6065,  6074,  6078,  6080,\n",
       "        6081,  6083,  6088,  6138,  6154,  6155,  6171,  6180,  6182,\n",
       "        6212,  6215,  6217,  6224,  6237,  6246,  6251,  6254,  6264,\n",
       "        6277,  6297,  6306,  6311,  6312,  6325,  6361,  6364,  6365,\n",
       "        6383,  6386,  6396,  6397,  6439,  6440,  6442,  6464,  6477,\n",
       "        6479,  6482,  6485,  6490,  6494,  6500,  6512,  6524,  6527,\n",
       "        6534,  6535,  6545,  6548,  6562,  6565,  6570,  6576,  6585,\n",
       "        6590,  6599,  6604,  6605,  6606,  6610,  6619,  6666,  6702,\n",
       "        6705,  6711,  6727, 11200, 11319, 11981, 11985, 11988, 11995,\n",
       "       12012, 12023, 12031, 12057, 12058, 12068, 12073, 12083, 12084,\n",
       "       12087, 12093, 12096, 12105, 12113, 12118, 12122, 12127, 12132,\n",
       "       12137, 12144, 12146, 12161, 12166, 12169, 12171, 12172, 12177,\n",
       "       12207, 12212, 12216, 12217, 12221, 12249, 12250, 12256, 12261,\n",
       "       12267, 12274, 12275, 12276, 12282, 12289, 12294, 12303, 12310,\n",
       "       12319, 12320, 12322, 12341, 12342, 12355, 12356, 12363, 12369,\n",
       "       12372, 12374, 12386, 12394, 12395, 12402, 12413, 12427, 12428])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The unique IDs of bikes for which the type is specified \n",
    "bike_ids_with_type = la_2018_extension[pd.isna(la_2018_extension[\"bike_type\"])==False][\"bike_id\"].unique()\n",
    "\n",
    "# The unique IDs of bikes for which the type isn't specified \n",
    "bike_ids_without_type = la_2018_extension[pd.isna(la_2018_extension[\"bike_type\"])][\"bike_id\"].unique()\n",
    "\n",
    "# If the arrays are equal, we can infer each individual bike_type\n",
    "print(np.array_equal(bike_ids_with_type, bike_ids_without_type))\n",
    "np.setdiff1d(bike_ids_without_type, bike_ids_with_type, assume_unique=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the Bike IDs and the Bike Type\n",
    "bike_type_by_id = la_2018_extension[pd.isna(la_2018_extension[\"bike_type\"])==False][[\"bike_id\",\"bike_type\"]]\n",
    "\n",
    "# Function for getting the corresponding Bike Type to an available Bike ID\n",
    "def getBikeType(id):\n",
    "    return bike_type_by_id[bike_type_by_id[\"bike_id\"]==id][\"bike_type\"].unique()\n",
    "\n",
    "# Function for filling out the missing Bike Types. If the Bike Type is available for a Bike ID, the Bike ID is returned.\n",
    "# If the Bike Type is missing for a Bike ID the \"standard\" Bike Type is returned\n",
    "def fillMissingBikeTypes(x):\n",
    "\n",
    "    if(pd.isna(x[\"bike_type\"])):\n",
    "        \n",
    "        if(x[\"bike_id\"] in bike_ids_with_type):\n",
    "            \n",
    "             return getBikeType(x[\"bike_id\"])[0]\n",
    "            \n",
    "        else: return \"standard\"\n",
    "        \n",
    "    return x[\"bike_type\"]\n",
    "\n",
    "# Filling out the missing Bike Types in our extension dataframe\n",
    "la_2018_extension[\"bike_type\"] = la_2018_extension.apply(lambda x: fillMissingBikeTypes(x), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 311894 entries, 0 to 311893\n",
      "Data columns (total 18 columns):\n",
      " #   Column               Non-Null Count   Dtype  \n",
      "---  ------               --------------   -----  \n",
      " 0   index                311894 non-null  int64  \n",
      " 1   trip_id              311894 non-null  int64  \n",
      " 2   duration             311894 non-null  int64  \n",
      " 3   start_time           311894 non-null  object \n",
      " 4   end_time             311894 non-null  object \n",
      " 5   start_station        311894 non-null  int64  \n",
      " 6   start_lat            311146 non-null  float64\n",
      " 7   start_lon            311146 non-null  float64\n",
      " 8   end_station          311894 non-null  int64  \n",
      " 9   end_lat              306771 non-null  float64\n",
      " 10  end_lon              306771 non-null  float64\n",
      " 11  bike_id              311894 non-null  int64  \n",
      " 12  plan_duration        311894 non-null  int64  \n",
      " 13  trip_route_category  311894 non-null  object \n",
      " 14  passholder_type      311894 non-null  object \n",
      " 15  bike_type            311894 non-null  object \n",
      " 16  start_coordinates    311894 non-null  object \n",
      " 17  end_coordinates      311894 non-null  object \n",
      "dtypes: float64(4), int64(7), object(7)\n",
      "memory usage: 42.8+ MB\n"
     ]
    }
   ],
   "source": [
    "la_2018_extension.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the \"bike_type\" column does not contain null values anymore, we can now add it to the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_2018_set[\"bike_type\"] = la_2018_extension[\"bike_type\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping outliers and redundant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>start_station</th>\n",
       "      <th>end_station</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>user_type</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>hour</th>\n",
       "      <th>week_day</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>trip_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>start_coordinates</th>\n",
       "      <th>end_coordinates</th>\n",
       "      <th>distance</th>\n",
       "      <th>km/h</th>\n",
       "      <th>bike_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 00:04:00</td>\n",
       "      <td>2018-01-01 00:25:00</td>\n",
       "      <td>3063</td>\n",
       "      <td>3018</td>\n",
       "      <td>5889</td>\n",
       "      <td>Walk-up</td>\n",
       "      <td>Pershing Square</td>\n",
       "      <td>Grand &amp; Olympic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>1</td>\n",
       "      <td>65406367</td>\n",
       "      <td>21</td>\n",
       "      <td>(34.0492, -118.2528)</td>\n",
       "      <td>(34.0437, -118.2601)</td>\n",
       "      <td>0.91</td>\n",
       "      <td>2.6</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 00:05:00</td>\n",
       "      <td>2018-01-01 00:25:00</td>\n",
       "      <td>3063</td>\n",
       "      <td>3018</td>\n",
       "      <td>6311</td>\n",
       "      <td>Walk-up</td>\n",
       "      <td>Pershing Square</td>\n",
       "      <td>Grand &amp; Olympic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>1</td>\n",
       "      <td>65406366</td>\n",
       "      <td>20</td>\n",
       "      <td>(34.0492, -118.2528)</td>\n",
       "      <td>(34.0437, -118.2601)</td>\n",
       "      <td>0.91</td>\n",
       "      <td>2.7</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 00:06:00</td>\n",
       "      <td>2018-01-01 00:25:00</td>\n",
       "      <td>3063</td>\n",
       "      <td>3018</td>\n",
       "      <td>5753</td>\n",
       "      <td>Walk-up</td>\n",
       "      <td>Pershing Square</td>\n",
       "      <td>Grand &amp; Olympic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>1</td>\n",
       "      <td>65406365</td>\n",
       "      <td>19</td>\n",
       "      <td>(34.0492, -118.2528)</td>\n",
       "      <td>(34.0437, -118.2601)</td>\n",
       "      <td>0.91</td>\n",
       "      <td>2.9</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 00:13:00</td>\n",
       "      <td>2018-01-01 00:35:00</td>\n",
       "      <td>3018</td>\n",
       "      <td>3031</td>\n",
       "      <td>6220</td>\n",
       "      <td>Monthly Pass</td>\n",
       "      <td>Grand &amp; Olympic</td>\n",
       "      <td>7th &amp; Spring</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>1</td>\n",
       "      <td>65406364</td>\n",
       "      <td>22</td>\n",
       "      <td>(34.0437, -118.2601)</td>\n",
       "      <td>(34.0447, -118.2524)</td>\n",
       "      <td>0.72</td>\n",
       "      <td>2.0</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 00:14:00</td>\n",
       "      <td>2018-01-01 00:59:00</td>\n",
       "      <td>4204</td>\n",
       "      <td>4216</td>\n",
       "      <td>12436</td>\n",
       "      <td>Monthly Pass</td>\n",
       "      <td>Washington &amp; Abbot Kinney</td>\n",
       "      <td>17th St / SMC E Line Station</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>1</td>\n",
       "      <td>65406362</td>\n",
       "      <td>45</td>\n",
       "      <td>(33.9884, -118.4516)</td>\n",
       "      <td>(34.0234, -118.4796)</td>\n",
       "      <td>4.67</td>\n",
       "      <td>6.2</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           start_time            end_time  start_station  end_station  \\\n",
       "0 2018-01-01 00:04:00 2018-01-01 00:25:00           3063         3018   \n",
       "1 2018-01-01 00:05:00 2018-01-01 00:25:00           3063         3018   \n",
       "2 2018-01-01 00:06:00 2018-01-01 00:25:00           3063         3018   \n",
       "3 2018-01-01 00:13:00 2018-01-01 00:35:00           3018         3031   \n",
       "4 2018-01-01 00:14:00 2018-01-01 00:59:00           4204         4216   \n",
       "\n",
       "   bike_id     user_type         start_station_name  \\\n",
       "0     5889       Walk-up            Pershing Square   \n",
       "1     6311       Walk-up            Pershing Square   \n",
       "2     5753       Walk-up            Pershing Square   \n",
       "3     6220  Monthly Pass            Grand & Olympic   \n",
       "4    12436  Monthly Pass  Washington & Abbot Kinney   \n",
       "\n",
       "               end_station_name  hour  week_day         day  month   trip_id  \\\n",
       "0               Grand & Olympic     0         0  01/01/2018      1  65406367   \n",
       "1               Grand & Olympic     0         0  01/01/2018      1  65406366   \n",
       "2               Grand & Olympic     0         0  01/01/2018      1  65406365   \n",
       "3                  7th & Spring     0         0  01/01/2018      1  65406364   \n",
       "4  17th St / SMC E Line Station     0         0  01/01/2018      1  65406362   \n",
       "\n",
       "   duration     start_coordinates       end_coordinates  distance  km/h  \\\n",
       "0        21  (34.0492, -118.2528)  (34.0437, -118.2601)      0.91   2.6   \n",
       "1        20  (34.0492, -118.2528)  (34.0437, -118.2601)      0.91   2.7   \n",
       "2        19  (34.0492, -118.2528)  (34.0437, -118.2601)      0.91   2.9   \n",
       "3        22  (34.0437, -118.2601)  (34.0447, -118.2524)      0.72   2.0   \n",
       "4        45  (33.9884, -118.4516)  (34.0234, -118.4796)      4.67   6.2   \n",
       "\n",
       "  bike_type  \n",
       "0  standard  \n",
       "1  standard  \n",
       "2  standard  \n",
       "3  standard  \n",
       "4  standard  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la_2018_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop enteties with start- & end station 3000 as they don't represent customer rides\n",
    "la_2018_set.drop(la_2018_set[(la_2018_set[\"start_station\"]==3000)&(la_2018_set[\"end_station\"]==3000)].index, inplace=True)\n",
    "\n",
    "#Drop rides with > 25 km/h \n",
    "la_2018_set.drop(la_2018_set[la_2018_set[\"km/h\"]>25].index, inplace=True)\n",
    "\n",
    "#Drop start_station_name & end_station_name as we can identify the station by its ID \n",
    "la_2018_set.drop([\"start_station_name\", \"end_station_name\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>start_station</th>\n",
       "      <th>end_station</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>user_type</th>\n",
       "      <th>hour</th>\n",
       "      <th>week_day</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>trip_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>start_coordinates</th>\n",
       "      <th>end_coordinates</th>\n",
       "      <th>distance</th>\n",
       "      <th>km/h</th>\n",
       "      <th>bike_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 00:04:00</td>\n",
       "      <td>2018-01-01 00:25:00</td>\n",
       "      <td>3063</td>\n",
       "      <td>3018</td>\n",
       "      <td>5889</td>\n",
       "      <td>Walk-up</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>1</td>\n",
       "      <td>65406367</td>\n",
       "      <td>21</td>\n",
       "      <td>(34.0492, -118.2528)</td>\n",
       "      <td>(34.0437, -118.2601)</td>\n",
       "      <td>0.91</td>\n",
       "      <td>2.6</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 00:05:00</td>\n",
       "      <td>2018-01-01 00:25:00</td>\n",
       "      <td>3063</td>\n",
       "      <td>3018</td>\n",
       "      <td>6311</td>\n",
       "      <td>Walk-up</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>1</td>\n",
       "      <td>65406366</td>\n",
       "      <td>20</td>\n",
       "      <td>(34.0492, -118.2528)</td>\n",
       "      <td>(34.0437, -118.2601)</td>\n",
       "      <td>0.91</td>\n",
       "      <td>2.7</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 00:06:00</td>\n",
       "      <td>2018-01-01 00:25:00</td>\n",
       "      <td>3063</td>\n",
       "      <td>3018</td>\n",
       "      <td>5753</td>\n",
       "      <td>Walk-up</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>1</td>\n",
       "      <td>65406365</td>\n",
       "      <td>19</td>\n",
       "      <td>(34.0492, -118.2528)</td>\n",
       "      <td>(34.0437, -118.2601)</td>\n",
       "      <td>0.91</td>\n",
       "      <td>2.9</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 00:13:00</td>\n",
       "      <td>2018-01-01 00:35:00</td>\n",
       "      <td>3018</td>\n",
       "      <td>3031</td>\n",
       "      <td>6220</td>\n",
       "      <td>Monthly Pass</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>1</td>\n",
       "      <td>65406364</td>\n",
       "      <td>22</td>\n",
       "      <td>(34.0437, -118.2601)</td>\n",
       "      <td>(34.0447, -118.2524)</td>\n",
       "      <td>0.72</td>\n",
       "      <td>2.0</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 00:14:00</td>\n",
       "      <td>2018-01-01 00:59:00</td>\n",
       "      <td>4204</td>\n",
       "      <td>4216</td>\n",
       "      <td>12436</td>\n",
       "      <td>Monthly Pass</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>1</td>\n",
       "      <td>65406362</td>\n",
       "      <td>45</td>\n",
       "      <td>(33.9884, -118.4516)</td>\n",
       "      <td>(34.0234, -118.4796)</td>\n",
       "      <td>4.67</td>\n",
       "      <td>6.2</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           start_time            end_time  start_station  end_station  \\\n",
       "0 2018-01-01 00:04:00 2018-01-01 00:25:00           3063         3018   \n",
       "1 2018-01-01 00:05:00 2018-01-01 00:25:00           3063         3018   \n",
       "2 2018-01-01 00:06:00 2018-01-01 00:25:00           3063         3018   \n",
       "3 2018-01-01 00:13:00 2018-01-01 00:35:00           3018         3031   \n",
       "4 2018-01-01 00:14:00 2018-01-01 00:59:00           4204         4216   \n",
       "\n",
       "   bike_id     user_type  hour  week_day         day  month   trip_id  \\\n",
       "0     5889       Walk-up     0         0  01/01/2018      1  65406367   \n",
       "1     6311       Walk-up     0         0  01/01/2018      1  65406366   \n",
       "2     5753       Walk-up     0         0  01/01/2018      1  65406365   \n",
       "3     6220  Monthly Pass     0         0  01/01/2018      1  65406364   \n",
       "4    12436  Monthly Pass     0         0  01/01/2018      1  65406362   \n",
       "\n",
       "   duration     start_coordinates       end_coordinates  distance  km/h  \\\n",
       "0        21  (34.0492, -118.2528)  (34.0437, -118.2601)      0.91   2.6   \n",
       "1        20  (34.0492, -118.2528)  (34.0437, -118.2601)      0.91   2.7   \n",
       "2        19  (34.0492, -118.2528)  (34.0437, -118.2601)      0.91   2.9   \n",
       "3        22  (34.0437, -118.2601)  (34.0447, -118.2524)      0.72   2.0   \n",
       "4        45  (33.9884, -118.4516)  (34.0234, -118.4796)      4.67   6.2   \n",
       "\n",
       "  bike_type  \n",
       "0  standard  \n",
       "1  standard  \n",
       "2  standard  \n",
       "3  standard  \n",
       "4  standard  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la_2018_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset as pickle\n",
    "la_2018_set.to_pickle(\"Data/la_2018_set.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Dataset\n",
    "First, we import the weather-related data. Since we are only interested in the year 2018, we will exclude all data outside this frame. For this purpose, the \"date_time\" column is casted into datetime objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>max_temp</th>\n",
       "      <th>min_temp</th>\n",
       "      <th>precip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26280</th>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>14.4</td>\n",
       "      <td>13.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26281</th>\n",
       "      <td>2018-01-01 02:00:00</td>\n",
       "      <td>14.4</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26282</th>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>14.4</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26283</th>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>14.4</td>\n",
       "      <td>13.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26284</th>\n",
       "      <td>2018-01-01 05:00:00</td>\n",
       "      <td>13.9</td>\n",
       "      <td>13.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date_time  max_temp  min_temp  precip\n",
       "26280 2018-01-01 00:00:00      14.4      13.9     0.0\n",
       "26281 2018-01-01 02:00:00      14.4      14.4     0.0\n",
       "26282 2018-01-01 03:00:00      14.4      14.4     0.0\n",
       "26283 2018-01-01 04:00:00      14.4      13.9     0.0\n",
       "26284 2018-01-01 05:00:00      13.9      13.9     0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import weather dataset and cast to datetime objects\n",
    "weather_set = pd.read_csv(\"Data/weather_hourly_la.csv\")\n",
    "weather_set[\"date_time\"] = pd.to_datetime(weather_set[\"date_time\"])\n",
    "\n",
    "# Only keep data from the year 2018\n",
    "weather_set = weather_set[(weather_set['date_time']>=datetime(year=2018, month=1, day=1))&(weather_set['date_time']<datetime(year=2019, month=1, day=1))]\n",
    "weather_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8729 entries, 26280 to 35062\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   date_time  8729 non-null   datetime64[ns]\n",
      " 1   max_temp   8729 non-null   float64       \n",
      " 2   min_temp   8729 non-null   float64       \n",
      " 3   precip     8729 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(3)\n",
      "memory usage: 341.0 KB\n"
     ]
    }
   ],
   "source": [
    "weather_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending the given weather related dataframe and filling missing values\n",
    "Normally, a year should have 8760 hours, so some entries are missing in the given dataset. Since there is no efficient way to insert entities in a particular row in the data frame, we need to extend the dataframe and reassign the values in the right place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of convenience we will reindex the dataset \n",
    "weather_set.index=range(8729)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe, which can then be expanded while the original dataframe remains untouched\n",
    "weather_copy = weather_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that there are no gaps in the dataset.\n",
    "\n",
    "t1 = timedelta(hours=1, minutes=0)\n",
    "\n",
    "for i in range(1,8729):\n",
    "    \n",
    "    weather_set[\"date_time\"].loc[i]=weather_set[\"date_time\"].loc[i-1]+t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that there are no more gaps in the data set, we only need to extend it to a full year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = timedelta(hours=1, minutes=0)\n",
    "for i in range(8729,8760):\n",
    "    \n",
    "    weather_set.loc[i]=None\n",
    "    weather_set[\"date_time\"].loc[i]=weather_set[\"date_time\"].loc[i-1]+t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>max_temp</th>\n",
       "      <th>min_temp</th>\n",
       "      <th>precip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018-01-01 14:00:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>2018-01-06 08:00:00</td>\n",
       "      <td>14.4</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2018-01-06 12:00:00</td>\n",
       "      <td>14.4</td>\n",
       "      <td>13.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>2018-01-06 15:00:00</td>\n",
       "      <td>14.4</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>2018-01-09 22:00:00</td>\n",
       "      <td>13.9</td>\n",
       "      <td>13.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8527</th>\n",
       "      <td>2018-12-24 05:00:00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8545</th>\n",
       "      <td>2018-12-24 00:00:00</td>\n",
       "      <td>15.6</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8555</th>\n",
       "      <td>2018-12-24 09:00:00</td>\n",
       "      <td>14.4</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8561</th>\n",
       "      <td>2018-12-24 15:00:00</td>\n",
       "      <td>12.8</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8569</th>\n",
       "      <td>2018-12-24 23:00:00</td>\n",
       "      <td>16.1</td>\n",
       "      <td>16.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date_time  max_temp  min_temp  precip\n",
       "23   2018-01-01 14:00:00      10.0      10.0     0.0\n",
       "137  2018-01-06 08:00:00      14.4      14.4     0.0\n",
       "141  2018-01-06 12:00:00      14.4      13.9     0.0\n",
       "144  2018-01-06 15:00:00      14.4      14.4     0.0\n",
       "223  2018-01-09 22:00:00      13.9      13.3     1.0\n",
       "...                  ...       ...       ...     ...\n",
       "8527 2018-12-24 05:00:00      15.0      15.0     0.0\n",
       "8545 2018-12-24 00:00:00      15.6      15.6     0.0\n",
       "8555 2018-12-24 09:00:00      14.4      14.4     0.0\n",
       "8561 2018-12-24 15:00:00      12.8      12.8     0.0\n",
       "8569 2018-12-24 23:00:00      16.1      16.1     0.0\n",
       "\n",
       "[107 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_copy[weather_copy.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are some duplicates in the dataset. Since these entities do not represent valuable data, we can delete the corresponding rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "weather_copy.drop_duplicates(subset=[\"date_time\"],keep=\"first\",inplace=True)\n",
    "weather_copy.index=range(8305)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe after dropping duplicates\n",
    "weather_copy.sort_values(by=[\"date_time\"],inplace=True)\n",
    "weather_copy.index=range(8305)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we moved the rows before, we now need to reassign the appropriate value to the entries. For the entries that do not have a matching row in the original data set, all values are given as null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If data in the row of the copy of the weather dataframe is the same as the row in the original weather dataframe, data is reassigned to the original weather dataframe\n",
    "# If not, the corresponding rows in the original weather dataframe are filled with null values\n",
    "indexCopy=0\n",
    "indexOrig=0\n",
    "while indexCopy<8305 and indexOrig<8760:\n",
    "    \n",
    "    if (weather_copy[\"date_time\"].loc[indexCopy]==weather_set[\"date_time\"].loc[indexOrig]):\n",
    "        weather_set[\"max_temp\"].loc[indexOrig]=weather_copy[\"max_temp\"].loc[indexCopy]\n",
    "        weather_set[\"min_temp\"].loc[indexOrig]=weather_copy[\"min_temp\"].loc[indexCopy]\n",
    "        weather_set[\"precip\"].loc[indexOrig]=weather_copy[\"precip\"].loc[indexCopy]\n",
    "        indexCopy=indexCopy+1\n",
    "        indexOrig=indexOrig+1\n",
    "        \n",
    "    else:\n",
    "        weather_set[\"max_temp\"].loc[indexOrig]=None\n",
    "        weather_set[\"min_temp\"].loc[indexOrig]=None\n",
    "        weather_set[\"precip\"].loc[indexOrig]=None\n",
    "        indexOrig=indexOrig+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then fill in the remaining zero values with average values. This is especially easy for gaps (entries with null values) of one hour, since we can simply use the average values of the entries before and after the corresponding row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill gaps of one hour with the average values of the entries before and after\n",
    "for i in range(8760):\n",
    "    if weather_set[\"max_temp\"].isnull().loc[i]==True:\n",
    "        weather_set[\"max_temp\"].loc[i]=((weather_set[\"max_temp\"].loc[i+1]+weather_set[\"max_temp\"].loc[i-1])/2)\n",
    "        weather_set[\"min_temp\"].loc[i]=((weather_set[\"min_temp\"].loc[i+1]+weather_set[\"min_temp\"].loc[i-1])/2)\n",
    "        weather_set[\"precip\"].loc[i]=weather_set[\"precip\"].loc[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([2404, 2405, 3076, 3077, 3078, 3079, 3101, 3102, 4965, 4966, 5336,\n",
       "            5337, 5338, 5339, 5340, 5672, 5673, 5674, 5675, 5676, 7473, 7474,\n",
       "            7475, 7476, 7477, 7478, 7479, 7480],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexes in the original dataframe where the max_temp is null\n",
    "null_indexes = weather_set[weather_set[\"max_temp\"].isnull()].index\n",
    "null_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the rows with null values: Replace null values with the average values of the entries before and after, that have actual values and not null\n",
    "for index in null_indexes:\n",
    "    \n",
    "    if(pd.isna(weather_set.loc[index][\"max_temp\"])):\n",
    "        \n",
    "        not_nan_index = index\n",
    "        \n",
    "        while(pd.isna(weather_set.loc[not_nan_index][\"max_temp\"])):\n",
    "            not_nan_index = not_nan_index+1\n",
    "            \n",
    "        distance = not_nan_index - index\n",
    "        \n",
    "        max_temp_before = weather_set.loc[index-1][\"max_temp\"]\n",
    "        max_temp_after = weather_set.loc[index+distance][\"max_temp\"]\n",
    "        max_temp_step = (max_temp_before-max_temp_after)/(distance+1)\n",
    "        \n",
    "        min_temp_before = weather_set.loc[index-1][\"min_temp\"]\n",
    "        min_temp_after = weather_set.loc[index+distance][\"min_temp\"]\n",
    "        min_temp_step = (min_temp_before-min_temp_after)/(distance+1)\n",
    "        \n",
    "        precip = weather_set.loc[index-1][\"precip\"]\n",
    "        \n",
    "        for indexes in range(distance):\n",
    "            weather_set[\"max_temp\"].loc[index+indexes] = (weather_set[\"max_temp\"].loc[index-1] - max_temp_step * (indexes+1)).round(1)\n",
    "            weather_set[\"min_temp\"].loc[index+indexes] = (weather_set[\"min_temp\"].loc[index-1] - min_temp_step * (indexes+1)).round(1)\n",
    "            weather_set[\"precip\"].loc[index+indexes] = precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8760 entries, 0 to 8759\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   date_time  8760 non-null   datetime64[ns]\n",
      " 1   max_temp   8760 non-null   float64       \n",
      " 2   min_temp   8760 non-null   float64       \n",
      " 3   precip     8760 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(3)\n",
      "memory usage: 662.2 KB\n"
     ]
    }
   ],
   "source": [
    "weather_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the weather dataframe has 8760 values as desired and does not conain null values anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional date related Features\n",
    "As we want to gain deep insight into the Bikesharing business at specific timeframes we want to generate additional date related features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>max_temp</th>\n",
       "      <th>min_temp</th>\n",
       "      <th>precip</th>\n",
       "      <th>hour</th>\n",
       "      <th>week_day</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>14.4</td>\n",
       "      <td>13.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "      <td>14.4</td>\n",
       "      <td>14.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 02:00:00</td>\n",
       "      <td>14.4</td>\n",
       "      <td>14.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>14.4</td>\n",
       "      <td>14.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>14.4</td>\n",
       "      <td>13.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_time  max_temp  min_temp  precip  hour  week_day         day  \\\n",
       "0 2018-01-01 00:00:00      14.4     13.90     0.0     0         0  01/01/2018   \n",
       "1 2018-01-01 01:00:00      14.4     14.15     0.0     1         0  01/01/2018   \n",
       "2 2018-01-01 02:00:00      14.4     14.40     0.0     2         0  01/01/2018   \n",
       "3 2018-01-01 03:00:00      14.4     14.40     0.0     3         0  01/01/2018   \n",
       "4 2018-01-01 04:00:00      14.4     13.90     0.0     4         0  01/01/2018   \n",
       "\n",
       "   month  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add some date related collumns to the dataset\n",
    "weather_set[\"hour\"] = weather_set[\"date_time\"].apply(lambda x: x.hour)\n",
    "weather_set[\"week_day\"] = weather_set[\"date_time\"].apply(lambda x: x.weekday())\n",
    "weather_set[\"day\"] = weather_set[\"date_time\"].apply(lambda x: x.strftime(\"%d/%m/%Y\"))\n",
    "weather_set[\"month\"] = weather_set[\"date_time\"].apply(lambda x: x.month)\n",
    "\n",
    "weather_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset as pickle\n",
    "weather_set.to_pickle(\"Data/weather_set.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
